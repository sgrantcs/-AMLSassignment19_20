{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basedir, image_paths\n",
    "basedir = '../Datasets/celeba'\n",
    "images_dir = os.path.join(basedir,'img')\n",
    "labels_filename = 'labels.csv'\n",
    "correct_labels = dict()\n",
    "detection_data = []\n",
    "detection_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Datasets/celeba/labels.csv', 'r') as file:\n",
    "    reader = csv.reader(file, delimiter = '\\t')\n",
    "    next(reader)\n",
    "    #reader = csv.DictReader(file, delimiter = '\\t')\n",
    "    for row in reader:\n",
    "        correct_labels[row[1]] = row\n",
    "    #print(correct_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detectors = [ cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\"),\n",
    "                  cv2.CascadeClassifier(\"haarcascade_frontalface_alt2.xml\"),\n",
    "                  cv2.CascadeClassifier(\"haarcascade_frontalface_alt.xml\"),\n",
    "                  cv2.CascadeClassifier(\"haarcascade_frontalface_alt_tree.xml\"), \n",
    "                  cv2.CascadeClassifier(\"haarcascade_profileface.xml\") ]\n",
    "\n",
    "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "smile_cascade = cv2.CascadeClassifier('haarcascade_smile.xml')\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8)) \n",
    "\n",
    "def detect_face(image_file):\n",
    "\n",
    "    frame = cv2.imread(image_file) #Open image\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #Convert image to grayscale\n",
    "        \n",
    "    #Detect face by going through different classifiers. \n",
    "    detected_face = []\n",
    "    \n",
    "    for face_detector in face_detectors:\n",
    "        \n",
    "        face = face_detector.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        \n",
    "        if ( len(face) == 1 ):\n",
    "            \n",
    "            detected_face = face \n",
    "            break \n",
    "\n",
    "    face_image = [] \n",
    "    smile_image =[]\n",
    "    \n",
    "    if len(detected_face) >0:\n",
    "\n",
    "        # Extract face\n",
    "        for (x, y, w, h) in detected_face: #get coordinates and size of rectangle containing face\n",
    "            #print(f\"face found in file: {image_file}\") \n",
    "            \n",
    "            gray = gray[y:y+h, x:x+w] #Cut the frame to size\n",
    "            try:\n",
    "                face_image = cv2.resize(gray, (178, 218)) #Resize face so all images have same size\n",
    "            except:\n",
    "                pass #If error, pass file\n",
    "            \n",
    "            #Detect eyes in the gray image\n",
    "            eye = eye_cascade.detectMultiScale(ri_grayscale, 1.2, 18) \n",
    "            for (x_eye, y_eye, w_eye, h_eye) in eye:\n",
    "                cv2.rectangle(ri_color,(x_eye, y_eye),(x_eye+w_eye, y_eye+h_eye), (0, 180, 60), 2) \n",
    "\n",
    "            #Detect a smile in the gray image \n",
    "            smiles = smile_cascade.detectMultiScale(ri_grayscale, 1.8, 15) \n",
    "            for (x_smile, y_smile, w_smile, h_smile) in smiles: \n",
    "                smile_image = cv2.rectangle(ri_color,(x_smile, y_smile),(x_smile+w_smile, y_smile+h_smile), (0, 0, 255), 2)\n",
    "                \n",
    "    return face_image, smile_image \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def detect_smiles(grayscale, img):\n",
    " #   face = face_image()\n",
    "  #  for (x_face, y_face, w_face, h_face) in face:\n",
    "   #     cv2.rectangle(img, (x_face, y_face), (x_face+w_face, y_face+h_face), (255, 130, 0), 2)\n",
    "    #    ri_grayscale = grayscale[y_face:y_face+h_face, x_face:x_face+w_face]\n",
    "     #   ri_color = img[y_face:y_face+h_face, x_face:x_face+w_face] \n",
    "        \n",
    "        #Detect eyes in the gray image\n",
    "      #  eye = eye_cascade.detectMultiScale(ri_grayscale, 1.2, 18) \n",
    "       # for (x_eye, y_eye, w_eye, h_eye) in eye:\n",
    "        #    cv2.rectangle(ri_color,(x_eye, y_eye),(x_eye+w_eye, y_eye+h_eye), (0, 180, 60), 2) \n",
    "        \n",
    "        #Detect a smile in the gray image \n",
    "        #smiles = smile_cascade.detectMultiScale(ri_grayscale, 1.8, 15) \n",
    "  \n",
    "        #for (x_smile, y_smile, w_smile, h_smile) in smiles: \n",
    "         #   cv2.rectangle(ri_color,(x_smile, y_smile),(x_smile+w_smile, y_smile+h_smile), (0, 0, 255), 2) \n",
    "    \n",
    "   #     for (x_smile, y_smile, w_smile, h_smile) in smile: \n",
    "    #        cv2.rectangle(ri_color,(x_smile, y_smile),(x_smile+w_smile, y_smile+h_smile), (0, 0, 255), 2)\n",
    "    #return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'face_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-54168f442e55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mgrayscale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mfinal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrayscale\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'smiling'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-e5b293904d3e>\u001b[0m in \u001b[0;36mdetection\u001b[1;34m(grayscale, img)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdetection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrayscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mface\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_face\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_face\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_face\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_face\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mface\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_face\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_face\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_face\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mw_face\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_face\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mh_face\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m130\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mri_grayscale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrayscale\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_face\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_face\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mh_face\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_face\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx_face\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mw_face\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'face_image' is not defined"
     ]
    }
   ],
   "source": [
    "detection = get_files(image_filelist)\n",
    "for item in detection:\n",
    "    image = detect_face(os.path.join(images_dir, item)) #open image\n",
    "    if len(image) >0:\n",
    "        detection_data.append(image) #append image array to the data list\n",
    "        \n",
    "correct = 0\n",
    "\n",
    "for image in prediction_data:\n",
    "        pred, conf = fishface.predict(image)\n",
    "        if pred == int(correct_labels[item][3]):\n",
    "            correct += 1\n",
    "\n",
    "model_accuracy = (100*correct)/len(prediction_data)\n",
    "print(\"Model accuracy : \" + str(model_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-28-950bb41ecdf7>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-28-950bb41ecdf7>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    if final = TRUE:\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "image_filelist = os.listdir(images_dir)\n",
    "predicted_labels = list()\n",
    "for file in image_filelist:\n",
    "    if file.lower().endswith('.jpg'):\n",
    "        print(f'processing file %s', file)\n",
    "        image_path = os.path.join(images_dir, file)\n",
    "        img = cv2.imread(image_path,1)\n",
    "        grayscale = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        final = detection(grayscale,img)\n",
    "        if final = TRUE:\n",
    "            #write a copy of an image into a new folder called Smiling\n",
    "            #write image name and value \"1\" into the predicted labels file\n",
    "        else:\n",
    "            #write a copy of an image into a new folder called NonSmiling\n",
    "            #write image name and value \"0\" into the predicted labels file\n",
    "\n",
    "            \n",
    "#TODO: Finish looping by using the detection function above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: Add a classifier. It will only classify an image as a \"smile\" if there is a smile, so this will be classified as \"1\", and if there is no smile, it will classify an image as \"non-smiling\", or \"0\". "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
