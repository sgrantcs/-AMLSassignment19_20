{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH TO ALL IMAGES\n",
    "#basedir, image_paths, target_size\n",
    "basedir = '../Datasets/cartoon_set'\n",
    "images_dir = os.path.join(basedir,'img')\n",
    "labels_filename = 'labels.csv'\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "\n",
    "# how to find frontal human faces in an image using 68 landmarks.  These are points on the face such as the corners of the mouth, along the eyebrows, on the eyes, and so forth.\n",
    "\n",
    "# The face detector we use is made using the classic Histogram of Oriented\n",
    "# Gradients (HOG) feature combined with a linear classifier, an image pyramid,\n",
    "# and sliding window detection scheme.  The pose estimator was created by\n",
    "# using dlib's implementation of the paper:\n",
    "# One Millisecond Face Alignment with an Ensemble of Regression Trees by\n",
    "# Vahid Kazemi and Josephine Sullivan, CVPR 2014\n",
    "# and was trained on the iBUG 300-W face landmark dataset (see https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/):\n",
    "#     C. Sagonas, E. Antonakos, G, Tzimiropoulos, S. Zafeiriou, M. Pantic.\n",
    "#     300 faces In-the-wild challenge: Database and results.\n",
    "#     Image and Vision Computing (IMAVIS), Special Issue on Facial Landmark Localisation \"In-The-Wild\". 2016.\n",
    "\n",
    "\n",
    "def shape_to_np(shape, dtype=\"int\"):\n",
    "    # initialize the list of (x, y)-coordinates\n",
    "    coords = np.zeros((shape.num_parts, 2), dtype=dtype)\n",
    "\n",
    "    # loop over all facial landmarks and convert them \n",
    "    # to a 2-tuple of (x, y)-coordinates\n",
    "    for i in range(0, shape.num_parts):\n",
    "        coords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "\n",
    "    # return the list of (x, y)-coordinates\n",
    "    return coords\n",
    "\n",
    "def rect_to_bb(rect):\n",
    "    # take a bounding predicted by dlib and convert it\n",
    "    # to the format (x, y, w, h) as we would normally do\n",
    "    # with OpenCV\n",
    "    x = rect.left()\n",
    "    y = rect.top()\n",
    "    w = rect.right() - x\n",
    "    h = rect.bottom() - y\n",
    "\n",
    "    # return a tuple of (x, y, w, h)\n",
    "    return (x, y, w, h)\n",
    "\n",
    "\n",
    "def run_dlib_shape(image):\n",
    "    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks\n",
    "    # load the input image, resize it, and convert it to grayscale\n",
    "    resized_image = image.astype('uint8')\n",
    "\n",
    "    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = gray.astype('uint8')\n",
    "\n",
    "    # detect faces in the grayscale image\n",
    "    rects = detector(gray, 1)\n",
    "    num_faces = len(rects)\n",
    "    \n",
    "    if num_faces == 0:\n",
    "        return None, resized_image\n",
    "\n",
    "    face_areas = np.zeros((1, num_faces))\n",
    "    face_shapes = np.zeros((136, num_faces), dtype=np.int64)\n",
    "\n",
    "    # loop over the face detections\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        # determine the facial landmarks for the face region, then\n",
    "        # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "        # array\n",
    "        temp_shape = predictor(gray, rect)\n",
    "        temp_shape = shape_to_np(temp_shape)\n",
    "\n",
    "        # convert dlib's rectangle to a OpenCV-style bounding box\n",
    "        # [i.e., (x, y, w, h)],\n",
    "        #   (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "        (x, y, w, h) = rect_to_bb(rect)\n",
    "        face_shapes[:, i] = np.reshape(temp_shape, [136])\n",
    "        face_areas[0, i] = w * h\n",
    "    # find largest face and keep\n",
    "    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])\n",
    "\n",
    "    return dlibout, resized_image\n",
    "    print(\"finished detecting faces\")\n",
    "\n",
    "def extract_features_labels():\n",
    "    \"\"\"\n",
    "    This function extracts the landmarks features for all images in the folder 'Datasets/cartoon_set'.\n",
    "    It also extracts the shape label for each image.\n",
    "    :return:\n",
    "        landmark_features:  an array containing 68 landmark points for each image in which a face was detected\n",
    "        shape_labels:      an array containing 5 face shape labels (0,1,2,3,4) for each image in\n",
    "                            which a face was detected\n",
    "    \"\"\"\n",
    "    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir)]\n",
    "    target_size = None\n",
    "    \n",
    "    labels_file = open(os.path.join(basedir, labels_filename), 'r')\n",
    "    lines = labels_file.readlines()\n",
    "    shape_labels = dict()\n",
    "    for line in lines[1:]:\n",
    "        columns = line.split('\\t')\n",
    "        shape_labels[columns[0]] = int(columns[2])\n",
    "    \n",
    "    if os.path.isdir(images_dir):\n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "        for img_path in image_paths:\n",
    "            #print(image_path)\n",
    "            file_name = img_path.split('.')[2].split('\\\\')[-1]\n",
    "            #print(file_name)\n",
    "\n",
    "            # load image\n",
    "            img = image.img_to_array(\n",
    "                image.load_img(img_path,\n",
    "                               target_size=target_size,\n",
    "                               interpolation='bicubic'))\n",
    "            \n",
    "            # flatten three channel color image\n",
    "            #color_features = img.flatten()\n",
    "            # combine color features into a single array\n",
    "            #flat_features = np.hstack(color_features)\n",
    "            #flat_features, _ = run_dlib_shape(img)\n",
    "            features, _ = run_dlib_shape(img)\n",
    "            \n",
    "            if features is not None:\n",
    "                all_features.append(features)\n",
    "                all_labels.append(shape_labels[file_name])\n",
    "\n",
    "    landmark_features = np.array(all_features)\n",
    "    shape_labels = (np.array(all_labels)) # keeps the 5 face shape labels, 0, 1, 2, 3 and 4\n",
    "    return landmark_features, shape_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_X_test2049\n",
      "num_X_train6146\n",
      "Accuracy: 0.20693020985846755\n"
     ]
    }
   ],
   "source": [
    "def get_data():\n",
    "\n",
    "    X, Y = extract_features_labels()\n",
    "    Y = np.array([Y, -(Y - 1)]).T\n",
    "    # Split into a training set and a test set using a stratified k fold\n",
    "    # Split into a training and testing set\n",
    "    #reducing dimensions via load_digits\n",
    "    #digits = load_digits()\n",
    "    #X = digits.data\n",
    "    #y = digits.target\n",
    "    #classifier.fit(X[:1000], y[:1000])\n",
    "    #predictions = classifier.predict(X[1000:])\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "    \n",
    "    #X_train = X[:100]\n",
    "    #Y_train = Y[:100]\n",
    "    #X_test = X[100:]\n",
    "    #Y_test = Y[100:]\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "# sklearn functions implementation\n",
    "def img_SVM(training_images, training_labels, test_images, test_labels):\n",
    "    classifier = svm.LinearSVC(C=C, max_iter=10000) \n",
    "    #classifier = svm.SVC(kernel='linear', C = 1.0)\n",
    "    #classifier = svm.SVC(kernel='rbf', C = 10.0, gamma=0.1)\n",
    "    classifier.fit(training_images, training_labels)\n",
    "    pred = classifier.predict(test_images)\n",
    "    print(\"Accuracy:\", accuracy_score(test_labels, pred))\n",
    "\n",
    "    #print(pred)\n",
    "    return pred\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = get_data()\n",
    "num_X_test = len(X_test)\n",
    "print(f'num_X_test{num_X_test}')\n",
    "num_X_train = len(X_train)\n",
    "print(f'num_X_train{num_X_train}')\n",
    "pred = img_SVM(X_train.reshape((num_X_train, 68*2)), list(zip(*Y_train))[0], X_test.reshape((num_X_test, 68*2)), list(zip(*Y_test))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
